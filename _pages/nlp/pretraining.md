---
categories: ["nlp"]
---

## ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators
- BERT mask LM makes generative prediction on 15% of the tokens.
- Change task to replace 15% of the tokens (Generator), and discriminator tells which of the tokens are replaced.


![Electra](electra.jpg)
